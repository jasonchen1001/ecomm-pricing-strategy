import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from data_preprocessing import load_data, extract_features
from price_elasticity import PriceElasticityAnalyzer
from sentiment_analysis import SentimentAnalyzer
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from wordcloud import WordCloud
from nltk.corpus import opinion_lexicon
import nltk

# å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(page_title="Amazon Product Analysis", layout="wide")

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
    <style>
    .main {
        padding: 2rem;
    }
    .stMetric {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .stPlotly {
        background-color: white;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    h1 {
        color: #1f77b4;
        padding-bottom: 1rem;
        border-bottom: 2px solid #1f77b4;
    }
    h2 {
        color: #2c3e50;
        margin-top: 2rem;
    }
    </style>
""", unsafe_allow_html=True)

# ç¡®ä¿ä¸‹è½½å¿…è¦çš„è¯å…¸
try:
    nltk.data.find('corpora/opinion_lexicon')
except LookupError:
    nltk.download('opinion_lexicon')

def normalize_sizes(sizes, min_size=8, max_size=40):
    """å°†è¯äº‘å­—ä½“å¤§å°å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´"""
    if len(sizes) == 0:
        return []
    min_val = min(sizes)
    max_val = max(sizes)
    if max_val == min_val:
        return [max_size] * len(sizes)
    return [min_size + (max_size - min_size) * (s - min_val) / (max_val - min_val) for s in sizes]

def get_color_gradient(word=None, font_size=None, position=None, orientation=None, font_path=None, random_state=None):
    """ä¸ºè¯äº‘ç”Ÿæˆæ¸å˜è‰²"""
    # ä¸ºç§¯æè¯„è®ºä½¿ç”¨ç»¿è‰²æ¸å˜
    positive_colors = ['#90EE90', '#32CD32', '#228B22', '#006400']  # æµ…ç»¿åˆ°æ·±ç»¿
    # ä¸ºæ¶ˆæè¯„è®ºä½¿ç”¨çº¢è‰²æ¸å˜
    negative_colors = ['#FFB6C1', '#FF6B6B', '#DC143C', '#8B0000']  # æµ…çº¢åˆ°æ·±çº¢
    
    # æ ¹æ®å­—ä½“å¤§å°é€‰æ‹©é¢œè‰²
    colors = positive_colors if random_state.randint(2) == 0 else negative_colors
    color_idx = int(font_size * (len(colors) - 1) / 100)
    return colors[min(color_idx, len(colors) - 1)]

def create_wordcloud(text, title, sentiment_type='positive'):
    """åˆ›å»ºè¯äº‘å›¾"""
    # è·å–NLTKçš„æƒ…æ„Ÿè¯å…¸
    positive_words = set(opinion_lexicon.positive())
    negative_words = set(opinion_lexicon.negative())
    
    # äº§å“è¯„è®ºç‰¹å®šçš„ç§¯æè¯
    product_specific_positive = {
        'affordable', 'worth', 'sturdy', 'durable', 'reliable', 
        'fast', 'quick', 'solid', 'perfect', 'excellent',
        'strong', 'stable', 'premium', 'professional', 'recommended',
        'satisfied', 'quality', 'great', 'nice', 'good',
        'convenient', 'efficient', 'effective', 'impressive'
    }
    
    # äº§å“è¯„è®ºç‰¹å®šçš„æ¶ˆæè¯
    product_specific_negative = {
        'defective', 'faulty', 'broken', 'useless',
        'disappointing', 'terrible', 'horrible', 'awful',
        'worthless', 'poor-quality', 'unreliable', 'unstable',
        'overpriced', 'ineffective', 'malfunctioning',
        'negative', 'strain'
    }
    
    # éœ€è¦ç§»é™¤çš„æ­§ä¹‰è¯
    ambiguous_words = {
        'quality',    # å¯èƒ½è¡¨ç¤ºå¥½(ç§¯æ)æˆ–å·®(æ¶ˆæ)
        'cheap',      # å¯èƒ½è¡¨ç¤ºä¾¿å®œ(ç§¯æ)æˆ–åŠ£è´¨(æ¶ˆæ)
        'hard',       # å¯èƒ½è¡¨ç¤ºåšç¡¬(ç§¯æ)æˆ–å›°éš¾(æ¶ˆæ)
        'emergency',  # æè¿°æƒ…å†µè€Œéäº§å“è´¨é‡
        'blame',      # æè¿°è¡Œä¸ºè€Œéäº§å“
        'basic',      # å¯èƒ½æ˜¯ä¸­æ€§æè¿°
        'simple',     # å¯èƒ½æ˜¯ç§¯ææˆ–æ¶ˆæ
        'just',       # é€šå¸¸æ˜¯ä¸­æ€§è¯
        'want',       # æ„æ„¿æè¿°
        'need',       # éœ€æ±‚æè¿°
        'try',        # è¡Œä¸ºæè¿°
        'return',     # è¡Œä¸ºæè¿°
        'cost',       # ä»·æ ¼æè¿°
        'price',      # ä»·æ ¼æè¿°
        'charge'      # å¯èƒ½æ˜¯å……ç”µæˆ–æ”¶è´¹
    }
    
    # åœç”¨è¯
    stop_words = {
        'issues', 'issue', 'problem', 'problems',  # ä¸­æ€§è¯
        'like', 'well', 'better', 'works', 'work',  # ä¸­æ€§/æè¿°æ€§è¯
        'cable', 'charger', 'wire', 'cord', 'adapter',  # äº§å“ç›¸å…³è¯
        'time', 'month', 'day', 'year', 'week',  # æ—¶é—´ç›¸å…³è¯
        'amazon', 'product', 'purchase', 'bought', 'order',  # è´­ä¹°ç›¸å…³è¯
        'use', 'using', 'used', 'usage',  # ä½¿ç”¨ç›¸å…³è¯
        'one', 'two', 'three', 'first', 'second',  # æ•°å­—ç›¸å…³è¯
        'the', 'and', 'for', 'that', 'this', 'with',  # å¸¸è§åœç”¨è¯
        'was', 'is', 'are', 'were', 'been', 'be', 'have'
    }
    
    # æ›´æ–°æƒ…æ„Ÿè¯å…¸
    positive_words.update(product_specific_positive)
    negative_words.update(product_specific_negative)
    
    # ç§»é™¤æ­§ä¹‰è¯å’Œåœç”¨è¯
    positive_words = positive_words - ambiguous_words - stop_words
    negative_words = negative_words - ambiguous_words - stop_words
    
    # åˆ†è¯å¹¶è¿‡æ»¤
    words = text.lower().split()
    if sentiment_type == 'positive':
        filtered_words = [word for word in words 
                         if word in positive_words]
        colormap = 'YlGn'
    else:
        filtered_words = [word for word in words 
                         if word in negative_words]
        colormap = 'RdPu'
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æƒ…æ„Ÿè¯ï¼Œè¿”å›ç©ºå›¾
    if not filtered_words:
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.text(0.5, 0.5, 'No significant sentiment words found',
                ha='center', va='center')
        ax.axis('off')
        return fig
    
    # ç”Ÿæˆè¯äº‘
    text = ' '.join(filtered_words)
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color='white',
        colormap=colormap,
        max_words=30,
        min_font_size=12,
        max_font_size=160,
        prefer_horizontal=0.7
    ).generate(text)
    
    # åˆ›å»ºå›¾å½¢ä½†ä¸æ·»åŠ æ ‡é¢˜
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig

def get_top_sentiment_words(text, sentiment_type='positive', n=10):
    """è·å–å‰Nä¸ªæœ€å¸¸è§çš„æƒ…æ„Ÿè¯"""
    # è·å–æƒ…æ„Ÿè¯å…¸
    positive_words = set(opinion_lexicon.positive())
    negative_words = set(opinion_lexicon.negative())
    
    # äº§å“è¯„è®ºç‰¹å®šçš„ç§¯æè¯
    product_specific_positive = {
        'affordable', 'worth', 'sturdy', 'durable', 'reliable', 
        'fast', 'quick', 'solid', 'perfect', 'excellent',
        'strong', 'stable', 'premium', 'professional', 'recommended',
        'satisfied', 'quality', 'great', 'nice', 'good',
        'convenient', 'efficient', 'effective', 'impressive'
    }
    
    # äº§å“è¯„è®ºç‰¹å®šçš„æ¶ˆæè¯ï¼ˆåªä¿ç•™æ˜ç¡®çš„è´Ÿé¢è¯ï¼‰
    product_specific_negative = {
        'defective', 'faulty', 'broken', 'useless',
        'disappointing', 'terrible', 'horrible', 'awful',
        'worthless', 'poor-quality', 'unreliable', 'unstable',
        'overpriced', 'ineffective', 'malfunctioning'
    }
    
    # éœ€è¦ä»æƒ…æ„Ÿè¯å…¸ä¸­ç§»é™¤çš„æ­§ä¹‰è¯
    ambiguous_words = {
        'quality',    # å¯èƒ½è¡¨ç¤ºä¾¿å®œ(ç§¯æ)æˆ–åŠ£è´¨(æ¶ˆæ)
        'cheap',      # å¯èƒ½è¡¨ç¤ºä¾¿å®œ(ç§¯æ)æˆ–åŠ£è´¨(æ¶ˆæ)
        'hard',       # å¯èƒ½è¡¨ç¤ºåšç¡¬(ç§¯æ)æˆ–å›°éš¾(æ¶ˆæ)
        'emergency',  # æè¿°æƒ…å†µè€Œéäº§å“è´¨é‡
        'blame',      # æè¿°è¡Œä¸ºè€Œéäº§å“
        'basic',      # å¯èƒ½æ˜¯ä¸­æ€§æè¿°
        'simple',     # å¯èƒ½æ˜¯ç§¯ææˆ–æ¶ˆæ
        'just',       # é€šå¸¸æ˜¯ä¸­æ€§è¯
        'want',       # æ„æ„¿æè¿°
        'need',       # éœ€æ±‚æè¿°
        'try',        # è¡Œä¸ºæè¿°
        'return',     # è¡Œä¸ºæè¿°
        'cost',       # ä»·æ ¼æè¿°
        'price',      # ä»·æ ¼æè¿°
        'charge'      # å¯èƒ½æ˜¯å……ç”µæˆ–æ”¶è´¹
    }
    
    # é¢å¤–çš„åœç”¨è¯
    extra_stop_words = {
        'issues', 'issue', 'problem', 'problems',  # ä¸­æ€§è¯
        'like', 'well', 'better', 'works', 'work',  # ä¸­æ€§/æè¿°æ€§è¯
        'cable', 'charger', 'wire', 'cord', 'adapter',  # äº§å“ç›¸å…³è¯
        'time', 'month', 'day', 'year', 'week',  # æ—¶é—´ç›¸å…³è¯
        'amazon', 'product', 'purchase', 'bought', 'order',  # è´­ä¹°ç›¸å…³è¯
        'use', 'using', 'used', 'usage',  # ä½¿ç”¨ç›¸å…³è¯
        'one', 'two', 'three', 'first', 'second',  # æ•°å­—ç›¸å…³è¯
    }
    
    # æ›´æ–°æƒ…æ„Ÿè¯å…¸
    positive_words.update(product_specific_positive)
    negative_words.update(product_specific_negative)
    
    # ç§»é™¤æ‰€æœ‰æ­§ä¹‰è¯å’Œåœç”¨è¯
    positive_words = positive_words - ambiguous_words - extra_stop_words
    negative_words = negative_words - ambiguous_words - extra_stop_words
    
    # åˆ†è¯å¹¶è¿‡æ»¤
    words = text.lower().split()
    word_freq = {}
    target_words = positive_words if sentiment_type == 'positive' else negative_words
    
    for word in words:
        if word in target_words:
            word_freq[word] = word_freq.get(word, 0) + 1
    
    # è·å–å‰Nä¸ªæœ€å¸¸è§çš„è¯
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:n]
    return top_words

def get_display_text(en_text, zh_text, lang='en'):
    """æ ¹æ®é€‰æ‹©çš„è¯­è¨€è¿”å›æ˜¾ç¤ºæ–‡æœ¬"""
    return en_text if lang == 'en' else zh_text

def main():
    # æ·»åŠ è¯­è¨€é€‰æ‹©å™¨åˆ°ä¾§è¾¹æ 
    lang = st.sidebar.selectbox(
        "Language / è¯­è¨€",
        options=['en', 'zh'],
        format_func=lambda x: "English" if x == 'en' else "ä¸­æ–‡"
    )
    
    try:
        df = load_data('amazon.csv')
    except Exception as e:
        st.error(get_display_text(
            'Error loading data: Please check if amazon.csv exists in the correct location.',
            'åŠ è½½æ•°æ®é”™è¯¯ï¼šè¯·æ£€æŸ¥ amazon.csv æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºæ­£ç¡®ä½ç½®ã€‚',
            lang
        ))
        st.exception(e)
        return
    
    st.title(get_display_text(
        'ğŸš— Amazon Product Analysis Dashboard',
        'ğŸš— äºšé©¬é€Šäº§å“åˆ†æä»ªè¡¨æ¿',
        lang
    ))
    
    # Market Overview éƒ¨åˆ†ç§»åˆ°è¿™é‡Œ
    st.header(get_display_text('ğŸ“Š Market Overview', 'ğŸ“Š å¸‚åœºæ¦‚è§ˆ', lang))
    
    # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            get_display_text("Total Products", "äº§å“æ€»æ•°", lang),
            f"{len(df):,}"
        )
    
    with col2:
        avg_rating = df['rating'].mean()
        st.metric(
            get_display_text("Average Rating", "å¹³å‡è¯„åˆ†", lang),
            f"{avg_rating:.2f} â­"
        )
    
    with col3:
        avg_price = df['discounted_price'].mean()
        st.metric(
            get_display_text("Average Price", "å¹³å‡ä»·æ ¼", lang),
            f"â‚¹{avg_price:.2f}"
        )
    
    with col4:
        avg_discount = df['real_discount'].mean()
        st.metric(
            get_display_text("Average Discount", "å¹³å‡æŠ˜æ‰£", lang),
            f"{avg_discount:.1f}%"
        )
    
    # åˆ›å»ºæ ‡ç­¾é¡µ - è°ƒæ•´é¡ºåº
    tab1, tab2, tab3 = st.tabs([
        get_display_text('Price Analysis', 'ä»·æ ¼åˆ†æ', lang),
        get_display_text('Review Analysis', 'è¯„è®ºåˆ†æ', lang),
        get_display_text('Product Rankings', 'äº§å“æ’å', lang)
    ])
    
    # åŠ è½½æ•°æ®æ—¶æ˜¾ç¤ºè¿›åº¦æ¡
    with st.spinner('Loading data...'):
        df, features, analyzer = load_cached_data()
    
    # åˆ›å»ºä»·æ ¼å¼¹æ€§åˆ†æå™¨å®ä¾‹
    elasticity_analyzer = PriceElasticityAnalyzer()
    
    # ä¾§è¾¹æ ä¼˜åŒ–
    st.sidebar.image('https://upload.wikimedia.org/wikipedia/commons/4/4a/Amazon_icon.svg', width=100)
    st.sidebar.title(get_display_text('Analysis Controls', 'åˆ†ææ§åˆ¶', lang))
    
    # ç®€åŒ–ä¾§è¾¹æ è®¾ç½®
    st.sidebar.markdown("---")
    st.sidebar.subheader(get_display_text('Price Elasticity Settings', 'ä»·æ ¼å¼¹æ€§è®¾ç½®', lang))
    
    # åªä¿ç•™å¼¹æ€§ç³»æ•°è®¡ç®—æ–¹æ³•é€‰æ‹©
    elasticity_method = st.sidebar.selectbox(
        get_display_text('Calculation Method', 'è®¡ç®—æ–¹æ³•', lang),
        ['Log-Log', 'Point', 'Arc'],
        help=get_display_text(
            "Method to calculate price elasticity",
            "è®¡ç®—ä»·æ ¼å¼¹æ€§çš„æ–¹æ³•",
            lang
        )
    )
    
    # æ·»åŠ æ›´å¤šç­›é€‰å™¨
    price_range = st.sidebar.slider(
        get_display_text('Price Range (â‚¹)', 'ä»·æ ¼èŒƒå›´ (â‚¹)', lang),
        float(df['discounted_price'].min()),
        float(df['discounted_price'].max()),
        (float(df['discounted_price'].min()), float(df['discounted_price'].max()))
    )
    
    rating_filter = st.sidebar.slider(
        get_display_text('Minimum Rating', 'æœ€ä½è¯„åˆ†', lang),
        min_value=0.0,
        max_value=5.0,
        value=0.0,
        step=0.5
    )
    
    # æ•°æ®ç­›é€‰
    mask = (
        (df['discounted_price'] >= price_range[0]) & 
        (df['discounted_price'] <= price_range[1]) &
        (df['rating'] >= rating_filter)
    )
    filtered_df = df[mask]
    
    # æ·»åŠ åˆ·æ–°æŒ‰é’®
    if st.sidebar.button(get_display_text('Refresh Analysis', 'åˆ·æ–°åˆ†æ', lang)):
        try:
            st.rerun()
        except AttributeError:
            try:
                st.experimental_rerun()
            except AttributeError:
                st.error(get_display_text(
                    "Refresh functionality not available in this Streamlit version",
                    "æ­¤ç‰ˆæœ¬çš„Streamlitä¸æ”¯æŒåˆ·æ–°åŠŸèƒ½",
                    lang
                ))
    
    # ä½¿ç”¨tabsç»„ç»‡å†…å®¹
    with tab1:
        # Price Analysis å†…å®¹
        #st.header(get_display_text('ğŸ’° Price Analysis', 'ğŸ’° ä»·æ ¼åˆ†æ', lang))
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader(get_display_text('ğŸ’° Price Distribution', 'ğŸ’° ä»·æ ¼åˆ†å¸ƒ', lang))
            fig = px.histogram(
                filtered_df,
                x='discounted_price',
                nbins=50,
                title=get_display_text(' Product Price Distribution', ' äº§å“ä»·æ ¼åˆ†å¸ƒ', lang)
            )
            
            # æ›´æ–°å›¾è¡¨å¸ƒå±€
            fig.update_layout(
                bargap=0.2,  # æ·»åŠ æŸ±å­ä¹‹é—´çš„é—´éš”
                plot_bgcolor='white',  # è®¾ç½®ç™½è‰²èƒŒæ™¯
                showlegend=False,
                xaxis=dict(
                    title='Price (â‚¹)',
                    gridcolor='lightgrey',
                    showgrid=True,
                    showline=True,
                    linewidth=1,
                    linecolor='black',
                    tickformat='â‚¹%d'
                ),
                yaxis=dict(
                    title='Number of Products',
                    gridcolor='lightgrey',
                    showgrid=True,
                    showline=True,
                    linewidth=1,
                    linecolor='black'
                ),
                margin=dict(l=40, r=40, t=40, b=40)  # è°ƒæ•´è¾¹è·
            )
            
            # æ›´æ–°æŸ±å­é¢œè‰²å’Œè¾¹æ¡†
            fig.update_traces(
                marker_color='rgb(30, 144, 255)',  # è®¾ç½®æŸ±å­é¢œè‰²ä¸ºæ·±è“è‰²
                marker_line_color='rgb(8, 48, 107)',  # è®¾ç½®è¾¹æ¡†é¢œè‰²
                marker_line_width=1  # è®¾ç½®è¾¹æ¡†å®½åº¦
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # æ·»åŠ ç›¸å…³æ€§åˆ†æ
            st.markdown("---")
            st.subheader(get_display_text('ğŸ“Š Correlation Analysis', 'ğŸ“Š ç›¸å…³æ€§åˆ†æ', lang))
            correlation_matrix = filtered_df[
                ['discounted_price', 'rating', 'rating_count', 'real_discount']
            ].corr()
            
            # åŒè¯­æ ‡ç­¾
            labels = {
                'discounted_price': get_display_text('Price', 'ä»·æ ¼', lang),
                'rating': get_display_text('Rating', 'è¯„åˆ†', lang),
                'rating_count': get_display_text('Reviews', 'è¯„è®ºæ•°', lang),
                'real_discount': get_display_text('Discount', 'æŠ˜æ‰£', lang)
            }
            
            fig = go.Figure(data=go.Heatmap(
                z=correlation_matrix,
                x=list(labels.values()),
                y=list(labels.values()),
                text=correlation_matrix.round(2),
                texttemplate='%{text}',
                textfont={"size": 10},
                hoverongaps=False,
                hovertemplate=get_display_text(
                    '%{x} vs %{y}<br>Correlation: %{z:.2f}',
                    '%{x} vs %{y}<br>ç›¸å…³æ€§: %{z:.2f}',
                    lang
                ) + '<extra></extra>',
                colorscale='RdBu',
                zmid=0
            ))
            
            fig.update_layout(
                title=get_display_text('Correlation Matrix', 'ç›¸å…³æ€§çŸ©é˜µ', lang),
                height=400,
                hoverlabel=dict(bgcolor="white"),
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader(get_display_text('ğŸ“ˆ Price Elasticity Analysis', 'ğŸ“ˆ ä»·æ ¼å¼¹æ€§åˆ†æ', lang))
            
            # è®¡ç®—ä»·æ ¼å¼¹æ€§ç³»æ•°
            elasticity = elasticity_analyzer.calculate_elasticity(
                filtered_df['discounted_price'].values,
                filtered_df['rating_count'].values,
                method=elasticity_method
            )
            
            # æ˜¾ç¤ºå¼¹æ€§ç³»æ•°åŠå…¶å«ä¹‰
            st.metric(get_display_text('Price Elasticity', 'ä»·æ ¼å¼¹æ€§ç³»æ•°', lang), f"{elasticity:.2f}")
            
            if elasticity < 0.5:
                st.success(get_display_text(
                    '''
                    **Low Price Elasticity** (< 0.5):
                    - Consumers are less sensitive to price changes
                    - Consider appropriate price increases
                    - Focus on product quality and brand building
                    ''',
                    '''
                    **ä½ä»·æ ¼å¼¹æ€§** (< 0.5):
                    - æ¶ˆè´¹è€…å¯¹ä»·æ ¼å˜åŒ–ä¸æ•æ„Ÿ
                    - å¯ä»¥è€ƒè™‘é€‚å½“æé«˜ä»·æ ¼
                    - é‡ç‚¹å…³æ³¨äº§å“è´¨é‡å’Œå“ç‰Œå»ºè®¾
                    ''',
                    lang
                ))
            else:
                st.warning(get_display_text(
                    '''
                    **High Price Elasticity** (â‰¥ 0.5):
                    - Consumers are sensitive to price changes
                    - Need careful price adjustments
                    - Pay attention to competitor pricing
                    ''',
                    '''
                    **é«˜ä»·æ ¼å¼¹æ€§** (â‰¥ 0.5):
                    - æ¶ˆè´¹è€…å¯¹ä»·æ ¼å˜åŒ–æ•æ„Ÿ
                    - éœ€è¦è°¨æ…è°ƒæ•´ä»·æ ¼
                    - å…³æ³¨ç«å“å®šä»·ç­–ç•¥
                    ''',
                    lang
                ))
            
            # æ ¹æ®ä¸åŒçš„è®¡ç®—æ–¹æ³•æ˜¾ç¤ºä¸åŒçš„ä»·æ ¼-éœ€æ±‚å…³ç³»å›¾
            if elasticity_method == 'Log-Log':
                # å¯¹æ•°è½¬æ¢åçš„æ•£ç‚¹å›¾
                fig = px.scatter(
                    filtered_df,
                    x='discounted_price',
                    y='rating_count',
                    title=get_display_text(
                        'Price-Demand Relationship',
                        'ä»·æ ¼-éœ€æ±‚å…³ç³»å›¾',
                        lang
                    ),
                    labels={
                        'discounted_price': get_display_text('Price (â‚¹)', 'ä»·æ ¼ (â‚¹)', lang),
                        'rating_count': get_display_text('Demand (log)', 'éœ€æ±‚é‡ (å¯¹æ•°)', lang)
                    }
                )
                
                # æ·»åŠ è¶‹åŠ¿çº¿
                fig.add_traces(go.Scatter(
                    x=filtered_df['discounted_price'],
                    y=filtered_df['rating_count'].mean() * np.ones(len(filtered_df)),
                    mode='lines',
                    name=get_display_text('Trend Line', 'è¶‹åŠ¿çº¿', lang),
                    line=dict(color='red', dash='dash')
                ))
            elif elasticity_method == 'Point':
                # åˆ†æ®µç‚¹å¼¹æ€§å›¾
                fig = px.scatter(
                    filtered_df,
                    x='discounted_price',
                    y='rating_count',
                    title='Point Price Elasticity',
                    labels={
                        'discounted_price': 'Price (â‚¹)',
                        'rating_count': 'Demand (Reviews)'
                    }
                )
                # æ·»åŠ åˆ†æ®µç‚¹å¼¹æ€§çº¿
                sorted_df = filtered_df.sort_values('discounted_price')
                segments = np.array_split(sorted_df, 5)
                for segment in segments:
                    fig.add_trace(go.Scatter(
                        x=segment['discounted_price'],
                        y=segment['rating_count'],
                        mode='lines',
                        name=f'Segment {len(fig.data)}'
                    ))
            else:  # Arc
                # å¼§å¼¹æ€§å›¾
                fig = px.scatter(
                    filtered_df,
                    x='discounted_price',
                    y='rating_count',
                    title='Arc Price Elasticity',
                    labels={
                        'discounted_price': 'Price (â‚¹)',
                        'rating_count': 'Demand (Reviews)'
                    },
                    trendline="lowess"  # ä½¿ç”¨å±€éƒ¨åŠ æƒå›å½’
                )
            
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        # Review Analysis æ ‡ç­¾é¡µ
        st.subheader(get_display_text('Sentiment Analysis', 'æƒ…æ„Ÿåˆ†æ', lang))
        
        # è·å–æƒ…æ„Ÿåˆ†ææ‘˜è¦
        sentiment_summary = analyzer.get_sentiment_summary(filtered_df, lang)
        
        # æ˜¾ç¤ºæƒ…æ„Ÿåˆ†å¸ƒ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                get_display_text('Positive Reviews', 'æ­£é¢è¯„è®º', lang),
                f"{sentiment_summary['positive_pct']:.1f}%"
            )
        with col2:
            st.metric(
                get_display_text('Neutral Reviews', 'ä¸­æ€§è¯„è®º', lang),
                f"{sentiment_summary['neutral_pct']:.1f}%"
            )
        with col3:
            st.metric(
                get_display_text('Negative Reviews', 'è´Ÿé¢è¯„è®º', lang),
                f"{sentiment_summary['negative_pct']:.1f}%"
            )
        
        st.plotly_chart(sentiment_summary['distribution_plot'], use_container_width=True)
        
        # æ˜¾ç¤ºTopæƒ…æ„Ÿè¯
        col1, col2 = st.columns(2)
        with col1:
            pos_words = analyzer.get_top_sentiment_words(filtered_df, 'positive', 10, lang)
            st.plotly_chart(pos_words['plot'], use_container_width=True)
        
        with col2:
            neg_words = analyzer.get_top_sentiment_words(filtered_df, 'negative', 10, lang)
            st.plotly_chart(neg_words['plot'], use_container_width=True)
    
    with tab3:
        # Product Rankings å†…å®¹
        st.header(get_display_text('ğŸ† Product Rankings', 'ğŸ† äº§å“æ’å', lang))
        top_products = filtered_df.nlargest(10, 'rating')[
            ['product_name', 'discounted_price', 'rating', 'rating_count']
        ].reset_index(drop=True)
        
        # ä½¿ç”¨æ›´å¥½çš„è¡¨æ ¼å±•ç¤º
        st.dataframe(
            top_products,
            column_config={
                "product_name": "Product Name",
                "discounted_price": st.column_config.NumberColumn(
                    get_display_text("Price (â‚¹)", "ä»·æ ¼ (â‚¹)", lang),
                    format="â‚¹%.2f"
                ),
                "rating": st.column_config.NumberColumn(
                    get_display_text("Rating", "è¯„åˆ†", lang),
                    format="%.1f â­"
                ),
                "rating_count": st.column_config.NumberColumn(
                    get_display_text("Reviews", "è¯„è®º", lang),
                    format="%d ğŸ“"
                )
            },
            hide_index=True,
            use_container_width=True
        )
    
    # ç§»é™¤é¡µé¢åº•éƒ¨çš„ç›¸å…³æ€§çŸ©é˜µ
    # åªä¿ç•™é¡µè„š
    st.markdown("""---""")
    st.markdown("""
        <div style='text-align: center'>
            <p>Made with â¤ï¸ by Yanzhen Chen | Data last updated: 2025</p>
        </div>
    """, unsafe_allow_html=True)

# åŠ è½½æ•°æ®
@st.cache_data
def load_cached_data():
    df = load_data('amazon.csv')
    features = extract_features(df)
    
    # æƒ…æ„Ÿåˆ†æ
    analyzer = SentimentAnalyzer()
    df = analyzer.analyze_reviews(df)
    
    return df, features, analyzer

if __name__ == '__main__':
    main() 